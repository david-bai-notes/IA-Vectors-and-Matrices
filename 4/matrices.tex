\section{Matrices and Linear Maps}
\subsection{Definitions}
\begin{definition}
    A linear map is a function that preserves linear combination.
    That is, for vector spaces $V,W$ over the same field $k$, a linear map $T:V\to W$ satisfies
    $$\forall\lambda,\mu\in k, \underline{v},\underline{v'}\in V, T(\lambda\underline{v}+\mu\underline{v'})=\lambda T(\underline{v})+\mu T(\underline{v'})$$
\end{definition}
\begin{definition}
    The image of the entire vector space $V$ under $T$, $\operatorname{Im}T$, is the collection of all images, that is, $\{\underline{w}\in W: \exists \underline{v}\in V,T(\underline{v})=\underline{w}\}$.
    Also, the kernel $\ker T$ is the set $\{\underline{v}\in V: T(\underline{v}=\underline{0})\}$
\end{definition}
\begin{proposition}
    The kernel is a subspace of $V$ and the image a subspace of $W$.
\end{proposition}
\begin{definition}
    The dimension of the image is called the rank of $T$, $\operatorname{rank}T$, and the dimension of the kernel is the nullity of $T$, $\operatorname{null}T$.
\end{definition}
\begin{example}
    1. The zero linear map $T$ mapping each vector to the zero vector is a linear.
    Its rank is $0$ and the nullity of $T$ is the dimension of $V$.\\
    2. The identity map on $V$ is linear with kernel $\{0\}$ and the image $V$.\\
    3. Suppose $V=W=\mathbb R^3$ and the map $T$ given by
    $$
    T\underline{x}=
    \begin{pmatrix}
        3&1&5\\
        -1&0&-2\\
        2&1&3
    \end{pmatrix}
    \underline{x}
    $$
    is linear.
    Note that this matrix is singular, so its kernel is nontrivial since it contains at least one vector, say $(2,-1,-1)$.
    One can show that the nullity is $1$ (that is, it is entirely generated by this vector) and the rank is $2$.
\end{example}
\begin{definition}
    If $T,S:V\to W$ are both linear maps, then $\alpha T+\beta S$ for any $\alpha,\beta\in F$ is obviously also a linear map.
    We say it is the linear combination of the linear maps.
\end{definition}
\begin{definition}
    If $T:V\to W, S:U\to V$ are both linear, then easily $T\circ S:U\to W$ is linear and is called the composition of linear maps.
\end{definition}
The third example shown above triggers the following theorem:
\begin{theorem}[Rank-Nullity Theorem]
    Suppose $T:V\to W$ is linear, then $\operatorname{rank}T+\operatorname{null}T=\dim V$.
\end{theorem}
\begin{proof}
    Let $\underline{e_1},\ldots,\underline{e_k}$ be a basis for $\ker T$.
    We can extend it to a basis $\underline{e_1},\ldots,\underline{e_n}$ of $V$.
    Now $T(\underline{e_{k+1}}),\ldots,T(\underline{e_n})$ is a basis for $\operatorname{Im}T$, following which the theorem is proved.
    Indeed,
    $$T\left(\sum_{i=1}^na_i\underline{e_i}\right)=\sum_{i=1}^na_iT(\underline{e_i})=\sum_{i=k+1}^na_iT(\underline{e_i})$$
    so this set indeed spans $\operatorname{Im}T$.
    It is also linearly independent since
    $$\sum_{i=k+1}^na_iT(\underline{e_i})=0\implies T\left(\sum_{i=k+1}^na_i\underline{e_i}\right)=0\implies \sum_{i=k+1}^na_i\underline{e_i}=\sum_{i=1}^ka_i\underline{e_i}$$
    But that would imply that we have found a nontrivial relation between our $\underline{e_i}$ which are independent, which is false.
    So it is independent, hence the proof is done.
\end{proof}
\subsection{Matrices as Linear Maps in Real Vector Spaces}
Let $M$ be an $n\times n$ array with entries $M_{ij}$ where $i$ labels the rows and $j$ labels the columns.
For example, if $n=3$,
$$
M=
\begin{pmatrix}
    M_{11}&M_{12}&M_{13}\\
    M_{21}&M_{22}&M_{23}\\
    M_{31}&M_{32}&M_{33}
\end{pmatrix}
$$
We define the map $T:\mathbb R^n\to\mathbb R^n$ by $T(\underline{x})_i=M_{ij}x_j$.
It is obviously a linear map.
Note that if $\underline{x}=x_i\underline{e_i}$, then $T(\underline{x})=x_iT(\underline{e_i})=x_i\underline{c_i}$ where $\underline{c_i}$ is the column $i$ of the matrix.
Therefore the image of $T$ is the span of the columns of $M$.\\
Given that, it is useful to consider the rows $\underline{r_i}$ and $\underline{c_i}$ that are the rows and columns of $M$.
We can write $(\underline{R_i})_j=M_{ij}=(\underline{C_j})_i$.
So $(M\underline{x})_i=\underline{R_i}\cdot\underline{x}$, the kernel of $M$ is the set of all $\underline{x}$ that vanishes under the linear map.
\begin{example}
    1. $V=W=\mathbb R^n$, the zero map corresponds to the zero matrix.\\
    2. The identity mao corresponds to the identity matrix $I_{ij}=\delta_{ij}$.\\
    3. $T:\mathbb R^3\to\mathbb R^3$ corresponding to the matrix
    $$M=\begin{pmatrix}
        3&1&5\\
        -1&0&-2\\
        2&1&3
    \end{pmatrix}$$
    The image of it is the span of columns, that is, the $2$-dimensional subspace spanned by $\underline{C_1},\underline{C_2}$ (since $\underline{C_3}$ is in this space).
    The kernel then is the $1$-dimensional subspace spanned by the vector $(2,-1,-1)$.
\end{example}
\subsection{Geometrical Examples}
We first think about rotations $\mathbb R^2\to\mathbb R^2$ by an angle $\theta$ can be described as
$$
\begin{pmatrix}
    \cos\theta&-\sin\theta\\
    \sin\theta&\cos\theta
\end{pmatrix}
$$
which one can eariy derive from either polar coordinate or the its behaviour on basis vectors.\\
Things go more interesting when we get to dimension $3$.
We consider $\underline{x}=\underline{x}_\parallel+\underline{x}_\perp$ as the decomposition of $\underline{x}$ along the $\underline{n}$ direction, i.e. such that $\underline{x}_\parallel\parallel\underline{n}$ and $\underline{x}_\perp\perp\underline{n}$.
Then,
$$|\underline{x}_\parallel|=|\underline{x}|\cos\phi,|\underline{x}_\perp|=|\underline{x}|\sin\phi$$
So under the rotation along the axis $\underline{n}$, $\underline{x}_\parallel$ stays the same while the $\underline{x}_\perp$ changes.
Assuming the angle is $\theta$, we can reassemble things to obtain
$$\underline{x}\mapsto \underline{x}_\parallel+\cos\theta\underline{x}_\perp+\sin\theta\underline{n}\times\underline{x}$$
So in components
\begin{align*}
    (M\underline{x})_i=M_{ij}x_j&=x_i\cos\theta+(1-\cos\theta)n_jx_jn_i+\sin\theta\epsilon_{ijk}n_jx_k\\
    &=(\delta_{ij}\cos\theta+(1-\cos\theta)n_in_j+\sin\theta\epsilon_{ijk}n_k)x_j
\end{align*}
We can also have reflection across the plane with normal $\underline{n}$ which, in matrix form, would be $M_{ij}=\delta_{ij}-2n_in_j$.\\
Dilation and scaling are linear maps as well.
If we set the scaling factors to be $\alpha,\beta,\gamma$ along $\underline{e_i},\underline{e_2},\underline{e_3}$, then the corresponding matrix would be
$$
\begin{pmatrix}
    \alpha&0&0\\
    0&\beta&0\\
    0&0&\gamma
\end{pmatrix}
$$
There is another kind of linear transformation called shears.
Given unit vectors $\underline{a},\underline{b}$ perpendicular, then a shear with parameter $\lambda$ is defined by $\underline{x}\mapsto \underline{x}+\lambda\underline{a}(\underline{b}\cdot\underline{x})$.
So $\underline{a}\mapsto\underline{a}$ (in general $\underline{u}\perp\underline{b}\implies \underline{u}\mapsto\underline{u}$), $\underline{b}\mapsto\underline{b}+\lambda\underline{a}$.
In component, $T(\underline{x})_i=(\delta_{ij}+\lambda a_ib_j)x_j$.
\subsection{Matrices in General; Matrix Algebra}
\begin{definition}
    Consider a linear map $T:V\to W$ where $V,W$ are real or complex vector spaces of dimensions $n,m$ respectively.
    Assume that we have obtained a basis $\{\underline{e_i}\}$ for $V$ and a basis $\{\underline{f_a}\}$ for $W$.
    So a matrix representation of $T$ with respect to these bases is an array $M_{ai}$ with entries in $\mathbb R,\mathbb C$ as appropriate, with $a\in\{1,2,\ldots,m\}$ (`rows'), $i\in\{1,2,\ldots,n\}$ (`columns') with
    $$T(\underline{e_i})=\sum_{a}M_{ai}\underline{f_a}$$
    which extends to any vectors in $V$ by linearity.
\end{definition}
So we have $M\underline{x}=T(\underline{x})=M_{ai}x_i\underline{f_a}$ where the summation convention is being used.
The moral of the story is that by choice with basis we can identify the vector spaces as $\mathbb R^n,\mathbb R^m$ or $\mathbb C^n,\mathbb C^m$ and $T$ as a matrix.
\begin{definition}
    Suppose $T:V\to W, S:V\to W, R:W\to Z$, then given choices of bases on $V,W,Z$, and hence the matrices $M$ of $T$, $N$ of $S$, $L$ of $R$, then $(M+N)_{ij}=M_{ij}+N_{ij}$ and $LM$ the matrix of $R\circ T$, or (as one can check) equivalently $(LM)_{ij}=L_{ia}M_{aj}$.
\end{definition}
One can also observe that the products of two matrices consists of exactly the dot products of the rows of one and the columns of the other.
This follows immediately from definitions.
Note that the matrix multiplication cannot be defined in two arbitrary matrices.
To multiply two matrices $A,B$ to get $AB$, we must have $\operatorname{dom}A=\operatorname{cod}B$.
\begin{proposition}
    For matrices where matrix multiplication is defined, it is associative and distributive over matrix addition.
\end{proposition}
\begin{proof}
    Trivial.
\end{proof}
\begin{definition}
    For $m\times n$ matrices $A$ and $n\times m$ matrices $B,C$, $B$ is a left inverse of $A$ if $BA=I$ and $C$ a right inverse of $A$ if $AC=I$ where $I$ is the identity.\\
    Note that not every matrices has inverse(s), but if it does, we say that it is invertible, or non-singular.
\end{definition}
\begin{proposition}
    If $n=m$, then if $A$ is invertible, then it has both an unique left inverse and an unique right inverse, and they are the same.
\end{proposition}
\begin{proof}
    Obvious enough.
\end{proof}
\begin{example}
    1. The rotation matrix $R(\theta)$ with respect to some (hyper-)axis has an inverse, since $R(\theta)\circ R(-\theta)=I$.\\
    2. Consider $n\times n$ matrix $M$.
    If $\underline{x'}=M\underline{x}$, then $M^{-1}\underline{x'}=\underline{x}$.
    This shows the uniqueness criterion of a system of linear equations.
\end{example}
One can check that a $2\times 2$ matrix $M$ is uniquely solvable if and only if its determinant $\det M=[M\underline{e_1},M\underline{e_2}]$ is nonzero.
One realize that the determinant is the area of the parallelogram spanned by the images of the bases.
\subsubsection{Transpose and Hermitian Conjugate}
If $M$ is an $m\times n$ matrix, then transpose, written $M^\top$ is the $n\times m$ defined by $(M^\top)_{ij}=M_{ji}$.
Note that for any two $n\times m$ matrices $M,N$ and scalars $\lambda,\mu$, then $(\lambda A+\mu B)^\top=\lambda A^\top+\mu B^\top$.
If $A$ is a $m\times n$ and $B$ is $n\times p$, then $(AB)^\top=B^\top A^\top$.
For square matrix $A$, we say $A$ is symmetric if $A^\top=A$, antisymmetric if $A^\top=-A$.\\
For complex matrices, the Hermitian conjugate of an $m\times n$ matrix $M$ is defined by $(M^\dagger)_{ij}=\overline{M_{ji}}$.
We can define Hermitian and anti-Hermitian matrices in the same way in square complex matrices.
\subsubsection{Trace}
Consider a complex $n\times n$ matrix $M$, the trace of $M$, $\operatorname{tr}(M)$, is $M_{ii}$ (where the summation convention is being used).
Immediately, $\operatorname{tr}(\alpha M+\beta N)=\alpha\operatorname{tr}(M)+\beta\operatorname{tr}(N)$, and $\operatorname{tr}(MN)=\operatorname{tr}(NM)$ and $\operatorname{tr}(M)=\operatorname{tr}(M^\top)$, and $\operatorname{tr}(I)=n$.
\begin{example}
    The reflection across the plane with normal $\underline{n}$ can be represented by the matrix $H$ defined by $H_{ij}=\delta_{ij}-2n_in_j$.
    Now $\operatorname{tr}(H)=H_{ii}=\delta_{ii}-2n_in_i=3-2=1$.
\end{example}
Note as well that antisymmetric matrices always have zero trace.
\subsubsection{Decomposition}
An $n\times n$ matrix $M$ can be written as $M=S+A$ where $S=(M+M^\top)/2,A=(M-M^\top)/2$.
Note that $S$ and $A$ are symmetric and antisymmetric respectively.
Consider the matrix $T$ defined by $T_{ij}=\delta_{ij}-\operatorname{tr}(S)\delta_{ij}/n$, so $T$ is traceless as well.
But $\operatorname{tr}S=\operatorname{tr}M$ since $A$ is antisymmetric hence traceless.
Thus $M_{ij}=T_{ij}+A_{ij}+\operatorname{tr}(M)\delta_{ij}/n$ decompose the matrix $M$ into symmetric traceless, antisymmetric and pure trace part.
\begin{example}
    For $n=3$, suppose $T_{ij}=0$, set $A_{ij}=\epsilon_{ijk}a_k$ and $\operatorname{tr}M=3\lambda$, so $M\underline{x}=\underline{x}\times\underline{a}+\lambda\underline{x}$.
\end{example}
\subsubsection{Orthogonal and Unitary Matrices}
A sqaure matrix $U$ is orthogonal if $UU^\top=U^\top U=I$.
So the column vectors of $U$ actually are orthonormal, same for the rows.
For example, rotation matrices are orthogonal.
\begin{proposition}
    $U$ is orthogonal if and only if it preserves inner products.
\end{proposition}
\begin{proof}
    For any square matrix $U$, we have
    $$(U\underline{x})\cdot(U\underline{y})=(U\underline{x})^\top U\underline{y}=\underline{x}^\top U^\top U\underline{y}$$
    So if $U$ is orthogonal, the last expression equals $\underline{x}\cdot\underline{y}$, hence $U$ preserves dot product.
    Conversely, for any square matrix $A$, $\underline{e_j}^\top A\underline{e_i}=A_{ji}$, so if $U$ preserves inner product, then by taking $A=U^\top U$ we have $U^\top U=I$
\end{proof}
Note that in $\mathbb R^n$, if $\{\underline{e_i}\}$ is an orthonormal basis, then $\{U\underline{e_i}\}$ would also be an orthonormal basis.
\begin{example}
    The general $2\times 2$ orthogonal matrices are all rotational or reflectional (from an axis through the origin) matrices.
    This can be trivially checked.
    In particular, reflections have determinant $-1$ while rotations always have determinant $1$.
\end{example}
\begin{definition}
    A complex $n\times n$ matrix $U$ is unitary if $U^\dagger U=UU^\dagger=I$.
    Equivalently, $U^\dagger=U^{-1}$.
\end{definition}
\begin{proposition}
    $U$ is unitary if and only if it preserves complex inner product.
\end{proposition}
\begin{proof}
    $(U\underline{x},U\underline{y})=\underline{x}^\dagger U^\dagger U\underline{y}$.
    Necessity is implied, and sufficiency is by $\underline{x}=\underline{e_i},\underline{y}=\underline{e_j}$.
\end{proof}
